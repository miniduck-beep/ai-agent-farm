# üîç –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ AI Agent Farm: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

## üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: 2 –æ–∫—Ç—è–±—Ä—è 2025

---

## üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏

### 1. **–°–ï–†–¨–Å–ó–ù–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: –î–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á (Celery vs BackgroundTasks)**

#### üîç –ü—Ä–æ–±–ª–µ–º–∞:
```python
# –í app/api.py - –õ–û–ì–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê
@app.post("/research", response_model=TaskResponse)
async def create_research_task(request: ResearchRequest, background_tasks: BackgroundTasks):
    # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç FastAPI BackgroundTasks
    background_tasks.add_task(
        run_research_task,  # –ù–æ —Ñ—É–Ω–∫—Ü–∏—è –∏–∑ tasks.py –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è Celery!
        task_id=task_id,
        topic=request.topic,
        crew_type=request.crew_type,
        language=request.language,
        depth=request.depth
    )
```

#### ‚ö†Ô∏è –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:
- Celery –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç —Å FastAPI BackgroundTasks
- –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏
- –ù–µ—Ç –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ
- –ó–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤ API –ø—Ä–æ—Ü–µ—Å—Å–µ

#### ‚úÖ –†–µ—à–µ–Ω–∏–µ:
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Celery task.delay() –∏–ª–∏ task.apply_async()

---

### 2. **–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: In-Memory —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ**

#### üîç –ü—Ä–æ–±–ª–µ–º–∞:
```python
# app/api.py - –ù–ï–ü–†–ò–ï–ú–õ–ï–ú–û –î–õ–Ø –ü–†–û–î–ê–ö–®–ï–ù–ê
tasks_storage: Dict[str, Dict] = {}  # In-memory storage –¥–ª—è –¥–µ–º–æ
```

#### ‚ö†Ô∏è –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:
- –ü–æ—Ç–µ—Ä—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ API
- –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
- –ü—Ä–æ–±–ª–µ–º—ã —Å –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å—é

---

### 3. **–ê–†–•–ò–¢–ï–ö–¢–£–†–ù–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π**

#### üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã:

**requirements.txt vs requirements_fixed.txt:**
```diff
# requirements.txt
fastapi==0.104.1          # –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è
uvicorn==0.24.0           # –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è
crewai==0.30.11           # –í–µ—Ä—Å–∏—è –±–µ–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏

# requirements_fixed.txt
fastapi>=0.110.0          # –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è (–ö–û–ù–§–õ–ò–ö–¢!)
uvicorn[standard]>=0.29.0 # –ù–æ–≤–∞—è –≤–µ—Ä—Å–∏—è (–ö–û–ù–§–õ–ò–ö–¢!)
crewai>=0.28.0            # –î—Ä—É–≥–∞—è –≤–µ—Ä—Å–∏—è (–ö–û–ù–§–õ–ò–ö–¢!)
```

#### ‚ö†Ô∏è –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:
- –ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø—Ä–∏ –¥–µ–ø–ª–æ–µ
- –í–æ–∑–º–æ–∂–Ω—ã–µ runtime –æ—à–∏–±–∫–∏
- –ü—Ä–æ–±–ª–µ–º—ã —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é

---

### 4. **–ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨: –û—Ç–∫—Ä—ã—Ç—ã–µ –ø–æ—Ä—Ç—ã –∏ —Å–ª–∞–±—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏**

#### üîç docker-compose.yml –ø—Ä–æ–±–ª–µ–º—ã:
```yaml
redis:
  ports:
    - "6379:6379"  # üö® –û–¢–ö–†–´–¢ –ù–ê–†–£–ñ–£ –ë–ï–ó –ê–£–¢–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–ò

xray:
  ports:
    - "10808:10808"  # üö® PROXY –î–û–°–¢–£–ü–ï–ù –ò–ó–í–ù–ï
    - "10809:10809"  # üö® –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–ê–Ø –î–´–†–ê
```

---

### 5. **–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–û–ù–ù–´–ï –û–®–ò–ë–ö–ò: –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å Dockerfile**

#### üîç –ü—Ä–æ–±–ª–µ–º–∞:
```dockerfile
# Dockerfile
FROM python:3.11-slim-bookworm
# –ù–æ –≤ docker-compose.prod.yml:
target: production  # üö® –≠–¢–û–ô –°–¢–ê–î–ò–ò –ù–ï–¢ –í DOCKERFILE!
```

---

### 6. **–ú–û–ù–ò–¢–û–†–ò–ù–ì: –ù–µ–ø–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –º–µ—Ç—Ä–∏–∫**

#### üîç –ü—Ä–æ–±–ª–µ–º—ã:
- Loki –∏ Grafana –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –ø—Ä–∏–≤—è–∑–∞–Ω–∞ –∫ –æ—Å–Ω–æ–≤–Ω—ã–º —Å–µ—Ä–≤–∏—Å–∞–º
- –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç health checks –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- –ù–µ—Ç Prometheus –º–µ—Ç—Ä–∏–∫
- –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∞–ª–µ—Ä—Ç—ã

---

## üí° –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è

### üèóÔ∏è **–†–µ—à–µ–Ω–∏–µ 1: Multi-stage Dockerfile —Å –ø–æ–ª–Ω–æ–π –∏–∑–æ–ª—è—Ü–∏–µ–π**

```dockerfile
# üöÄ –£–ª—É—á—à–µ–Ω–Ω—ã–π Dockerfile
FROM python:3.11-slim-bookworm AS base
WORKDIR /app

# –°–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
RUN apt-get update && apt-get install -y \
    curl \
    ca-certificates \
    build-essential \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –±–∞–∑–æ–≤—ã–µ
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# üß™ Development stage
FROM base AS development
COPY requirements-dev.txt .
RUN pip install --no-cache-dir -r requirements-dev.txt
COPY . .
CMD ["uvicorn", "app.api:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

# üöÄ Production stage  
FROM base AS production
COPY . .

# –°–æ–∑–¥–∞–µ–º non-root –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
RUN useradd --create-home --shell /bin/bash --uid 1001 app && \
    chown -R app:app /app
USER app

# Healthcheck
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["gunicorn", "app.api:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000"]

# üîß Worker stage
FROM production AS worker
CMD ["celery", "-A", "app.tasks", "worker", "--loglevel=info", "--concurrency=2"]

# üåê Web stage
FROM production AS web  
CMD ["streamlit", "run", "app/web_interface.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

---

### üèóÔ∏è **–†–µ—à–µ–Ω–∏–µ 2: –ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–∞—è Docker Compose –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**

```yaml
# docker-compose.microservices.yml
version: 3.8

services:
  # üöÄ API Gateway (–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
  api-gateway:
    build:
      context: .
      target: production
    container_name: ai-farm-api-gateway
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - SERVICE_NAME=api-gateway
      - REDIS_URL=redis://redis-cluster:6379/0
      - CELERY_BROKER_URL=redis://redis-cluster:6379/0
    networks:
      - api-network
      - redis-network
    depends_on:
      redis-cluster:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    resources:
      limits:
        cpus: 0.5
        memory: 512M
      reservations:
        cpus: 0.25
        memory: 256M

  # ‚öôÔ∏è Celery Workers (–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
  worker-general:
    build:
      context: .
      target: worker
    container_name: ai-farm-worker-general
    restart: unless-stopped
    environment:
      - SERVICE_NAME=worker-general
      - WORKER_TYPE=general
      - CELERY_QUEUES=general,research
    networks:
      - worker-network
      - redis-network
    depends_on:
      redis-cluster:
        condition: service_healthy
    deploy:
      replicas: 2
    resources:
      limits:
        cpus: 1.0
        memory: 1G

  worker-specialized:
    build:
      context: .
      target: worker
    container_name: ai-farm-worker-specialized
    environment:
      - SERVICE_NAME=worker-specialized
      - WORKER_TYPE=specialized
      - CELERY_QUEUES=swot,investment,technical_review
    networks:
      - worker-network
      - redis-network
    depends_on:
      redis-cluster:
        condition: service_healthy
    deploy:
      replicas: 1
    resources:
      limits:
        cpus: 2.0
        memory: 2G

  # üóÑÔ∏è Redis Cluster (–ø–æ–ª–Ω–∞—è –∏–∑–æ–ª—è—Ü–∏—è)
  redis-cluster:
    image: redis:7-alpine
    container_name: ai-farm-redis-cluster
    restart: unless-stopped
    command: >
      redis-server 
      --appendonly yes 
      --requirepass ${REDIS_PASSWORD:-secure_password}
      --maxmemory 1g 
      --maxmemory-policy allkeys-lru
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-secure_password}
    networks:
      - redis-network
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--pass", "${REDIS_PASSWORD:-secure_password}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    # –ù–ï–¢ –ü–£–ë–õ–ò–ß–ù–´–• –ü–û–†–¢–û–í - —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å–µ—Ç—å!

  # üåê Web Interface (–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
  web-interface:
    build:
      context: .
      target: web
    container_name: ai-farm-web
    restart: unless-stopped
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://api-gateway:8000
    networks:
      - web-network
      - api-network
    depends_on:
      api-gateway:
        condition: service_healthy
    resources:
      limits:
        cpus: 0.3
        memory: 256M

  # üîê VPN/Proxy (–∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
  secure-proxy:
    image: teddysun/xray:latest
    container_name: ai-farm-secure-proxy
    restart: unless-stopped
    volumes:
      - ./xray-config.json:/etc/xray/config.json:ro
    networks:
      - proxy-network
      - api-network
      - worker-network
    # –ù–ï–¢ –ü–£–ë–õ–ò–ß–ù–´–• –ü–û–†–¢–û–í - —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤

  # üìä Monitoring Stack (–ø–æ–ª–Ω–∞—è –∏–∑–æ–ª—è—Ü–∏—è)
  prometheus:
    image: prom/prometheus:v2.37.0
    container_name: ai-farm-prometheus
    restart: unless-stopped
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --web.console.templates=/etc/prometheus/consoles
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - monitoring-network
    # –ù–ï–¢ –ü–£–ë–õ–ò–ß–ù–´–• –ü–û–†–¢–û–í

  grafana:
    image: grafana/grafana:9.0.0
    container_name: ai-farm-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    networks:
      - monitoring-network
      - api-network
    depends_on:
      - prometheus

  loki:
    image: grafana/loki:2.8.0
    container_name: ai-farm-loki
    restart: unless-stopped
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki_data:/loki
      - ./logging/loki-config.yaml:/etc/loki/local-config.yaml
    networks:
      - monitoring-network

  promtail:
    image: grafana/promtail:2.8.0
    container_name: ai-farm-promtail
    restart: unless-stopped
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - ./logging/promtail-config.yaml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    networks:
      - monitoring-network

# üèóÔ∏è –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ç–∏
networks:
  api-network:
    driver: bridge
    name: ai-farm-api
  worker-network:
    driver: bridge
    name: ai-farm-workers
  redis-network:
    driver: bridge
    name: ai-farm-redis
    internal: true  # –¢–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å–≤—è–∑—å
  web-network:
    driver: bridge
    name: ai-farm-web
  proxy-network:
    driver: bridge
    name: ai-farm-proxy
    internal: true  # –¢–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å–≤—è–∑—å
  monitoring-network:
    driver: bridge
    name: ai-farm-monitoring
    internal: true  # –¢–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Å–≤—è–∑—å

# üì¶ –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
volumes:
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local
```

---

### üèóÔ∏è **–†–µ—à–µ–Ω–∏–µ 3: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ tasks.py**

```python
# app/tasks_fixed.py
"""
AI Agent Farm - Fixed Celery Tasks Architecture
==============================================
–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Celery –∏ Redis
"""

import json
import logging
from datetime import datetime
from typing import Optional, Dict, Any

import redis
from celery import Celery, Task
from celery.signals import worker_ready, worker_process_init
from celery.result import AsyncResult

from app.config import settings

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–Ω–∏–µ Celery –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
celery_app = Celery(
    "ai_agent_farm",
    broker=settings.celery_broker_url,
    backend=settings.celery_result_backend,
    include=["app.tasks_fixed"]
)

# –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Celery
celery_app.conf.update(
    # –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    result_expires=3600,
    
    # Timezone
    timezone="UTC",
    enable_utc=True,
    
    # Task routing and queues
    task_default_queue="default",
    task_routes={
        "app.tasks_fixed.run_research_task": {"queue": "research"},
        "app.tasks_fixed.run_swot_analysis": {"queue": "specialized"},
        "app.tasks_fixed.run_investment_analysis": {"queue": "specialized"},
        "app.tasks_fixed.run_technical_review": {"queue": "specialized"},
    },
    
    # Worker configuration
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    task_track_started=True,
    task_time_limit=3600,  # 1 —á–∞—Å
    task_soft_time_limit=3300,  # 55 –º–∏–Ω—É—Ç
    
    # Rate limiting
    task_annotations={
        "app.tasks_fixed.run_research_task": {"rate_limit": "10/m"},
        "app.tasks_fixed.run_swot_analysis": {"rate_limit": "5/m"},
        "app.tasks_fixed.run_investment_analysis": {"rate_limit": "5/m"},
        "app.tasks_fixed.run_technical_review": {"rate_limit": "5/m"},
    },
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    result_backend_transport_options={
        "master_name": "mymaster",
        "retry_policy": {
            "timeout": 5.0
        }
    }
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Redis –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
redis_client: Optional[redis.Redis] = None

@worker_process_init.connect
def init_worker(**kwargs):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è worker –ø—Ä–æ—Ü–µ—Å—Å–∞"""
    global redis_client
    redis_client = redis.Redis.from_url(settings.redis_url, decode_responses=True)
    logger.info("Worker process initialized with Redis connection")

class TaskManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∑–∞–¥–∞—á–∞–º–∏"""
    
    @staticmethod
    def get_task_status(task_id: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –∏–∑ Redis"""
        try:
            if not redis_client:
                raise RuntimeError("Redis client not initialized")
            
            task_data = redis_client.hgetall(f"task:{task_id}")
            if not task_data:
                return {"status": "not_found"}
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ –Ω—É–∂–Ω—ã–µ —Ç–∏–ø—ã
            if "progress" in task_data:
                task_data["progress"] = int(task_data["progress"])
            if "result" in task_data and task_data["result"]:
                task_data["result"] = json.loads(task_data["result"])
            
            return task_data
            
        except Exception as e:
            logger.error(f"Failed to get task status: {e}")
            return {"status": "error", "error": str(e)}
    
    @staticmethod
    def update_task_status(
        task_id: str, 
        status: str, 
        progress: Optional[int] = None,
        result: Optional[Dict] = None,
        error: Optional[str] = None
    ) -> bool:
        """–û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –≤ Redis"""
        try:
            if not redis_client:
                raise RuntimeError("Redis client not initialized")
            
            task_data = {
                "task_id": task_id,
                "status": status,
                "updated_at": datetime.utcnow().isoformat(),
            }
            
            if progress is not None:
                task_data["progress"] = progress
            if result is not None:
                task_data["result"] = json.dumps(result)
            if error is not None:
                task_data["error"] = error
            
            redis_client.hset(f"task:{task_id}", mapping=task_data)
            redis_client.expire(f"task:{task_id}", 86400)  # TTL 24 —á–∞—Å–∞
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to update task status: {e}")
            return False

class BaseAgentTask(Task):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç—Å–∫–∏—Ö –∑–∞–¥–∞—á"""
    
    def on_success(self, retval, task_id, args, kwargs):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
        TaskManager.update_task_status(task_id, "completed", progress=100, result=retval)
        logger.info(f"Task {task_id} completed successfully")
    
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–∫–∏"""
        TaskManager.update_task_status(task_id, "failed", error=str(exc))
        logger.error(f"Task {task_id} failed: {exc}")
    
    def on_retry(self, exc, task_id, args, kwargs, einfo):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫"""
        retry_count = self.request.retries
        TaskManager.update_task_status(
            task_id, 
            "retrying", 
            error=f"Retry {retry_count}: {str(exc)}"
        )
        logger.warning(f"Task {task_id} retry {retry_count}: {exc}")

# –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏
@celery_app.task(bind=True, base=BaseAgentTask, name="app.tasks_fixed.run_research_task")
def run_research_task(
    self, 
    task_id: str, 
    topic: str, 
    crew_type: str, 
    language: str = "ru", 
    depth: str = "standard"
) -> Dict[str, Any]:
    """–û—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    try:
        TaskManager.update_task_status(task_id, "processing", progress=10)
        self.update_state(state="PROGRESS", meta={"progress": 10, "status": "Initializing agents..."})
        
        # –ò–º–ø–æ—Ä—Ç –≤–Ω—É—Ç—Ä–∏ –∑–∞–¥–∞—á–∏
        from app.main_crew import CrewFactory
        
        factory = CrewFactory()
        TaskManager.update_task_status(task_id, "processing", progress=30)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        result = factory.run_research(
            topic=topic,
            crew_type=crew_type,
            language=language,
            depth=depth
        )
        
        TaskManager.update_task_status(task_id, "processing", progress=90)
        
        return {
            "task_id": task_id,
            "status": "completed",
            "result": result,
            "completed_at": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Task {task_id} failed: {e}")
        raise self.retry(exc=e, countdown=60, max_retries=3)

@celery_app.task(bind=True, base=BaseAgentTask, name="app.tasks_fixed.run_swot_analysis")
def run_swot_analysis(self, task_id: str, company_name: str, language: str = "ru") -> Dict[str, Any]:
    """SWOT –∞–Ω–∞–ª–∏–∑ –∫–æ–º–ø–∞–Ω–∏–∏"""
    try:
        TaskManager.update_task_status(task_id, "processing", progress=20)
        
        from app.main_crew import CrewFactory
        factory = CrewFactory()
        
        result = factory.create_swot_analyst_crew().kickoff({"company_name": company_name})
        
        return {
            "task_id": task_id,
            "analysis_type": "swot",
            "company_name": company_name,
            "result": result,
            "completed_at": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        raise self.retry(exc=e, countdown=30, max_retries=2)

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è API
def submit_research_task(topic: str, crew_type: str, language: str = "ru", depth: str = "standard") -> str:
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–¥–∞—á—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ Celery"""
    import uuid
    task_id = str(uuid.uuid4())
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ Redis –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –∑–∞–¥–∞—á–∏
    TaskManager.update_task_status(
        task_id=task_id,
        status="queued",
        progress=0
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º Celery –∑–∞–¥–∞—á—É
    run_research_task.apply_async(
        args=[task_id, topic, crew_type, language, depth],
        task_id=task_id
    )
    
    return task_id

def get_task_result(task_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–¥–∞—á–∏"""
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º Redis –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    task_status = TaskManager.get_task_status(task_id)
    
    # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º Celery —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    celery_result = AsyncResult(task_id, app=celery_app)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    return {
        "task_id": task_id,
        "status": task_status.get("status", celery_result.status.lower()),
        "progress": task_status.get("progress"),
        "result": task_status.get("result", celery_result.result),
        "error": task_status.get("error"),
        "celery_info": {
            "state": celery_result.state,
            "ready": celery_result.ready(),
            "successful": celery_result.successful() if celery_result.ready() else None
        }
    }

# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = [
    "celery_app",
    "submit_research_task", 
    "get_task_result",
    "run_research_task",
    "run_swot_analysis"
]
```

---

### üèóÔ∏è **–†–µ—à–µ–Ω–∏–µ 4: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π API —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Celery**

```python
# app/api_fixed.py
"""
AI Agent Farm - Fixed API with Proper Celery Integration
======================================================
–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è FastAPI —Å Celery
"""

import logging
from typing import Optional, Dict, Any

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from app.config import settings
from app.tasks_fixed import submit_research_task, get_task_result

logger = logging.getLogger(__name__)

app = FastAPI(
    title="ü§ñ AI Agent Farm API - Fixed",
    description="–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–æ–ª–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Celery",
    version="2.0.0-fixed"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ResearchRequest(BaseModel):
    topic: str = Field(..., min_length=5, max_length=500)
    crew_type: str = Field(default="general")
    language: str = Field(default="ru", pattern="^(ru|en)$")
    depth: str = Field(default="standard", pattern="^(basic|standard|comprehensive)$")

class TaskResponse(BaseModel):
    task_id: str
    status: str
    message: str

@app.post("/research", response_model=TaskResponse)
async def create_research_task(request: ResearchRequest):
    """–°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é Celery –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
        task_id = submit_research_task(
            topic=request.topic,
            crew_type=request.crew_type,
            language=request.language,
            depth=request.depth
        )
        
        logger.info(f"Submitted Celery task {task_id}: {request.topic}")
        
        return TaskResponse(
            task_id=task_id,
            status="queued",
            message="–ó–∞–¥–∞—á–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ –æ—á–µ—Ä–µ–¥—å Celery"
        )
        
    except Exception as e:
        logger.error(f"Failed to create task: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/result/{task_id}")
async def get_research_result(task_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–¥–∞—á–∏ (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    try:
        result = get_task_result(task_id)
        
        if result["status"] == "not_found":
            raise HTTPException(status_code=404, detail="Task not found")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get task result: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
    from app.tasks_fixed import celery_app
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Celery workers
        inspect = celery_app.control.inspect()
        active_workers = inspect.active()
        
        return {
            "status": "healthy",
            "celery_workers": len(active_workers) if active_workers else 0,
            "redis_connection": "ok"
        }
        
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"System unhealthy: {e}")
```

---

## üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —É–ª—É—á—à–µ–Ω–∏–π

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –î–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è | –ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è |
|-----------|---------------|-------------------|
| **–ó–∞–¥–∞—á–∏** | FastAPI BackgroundTasks | –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Celery —Å –æ—á–µ—Ä–µ–¥—è–º–∏ |
| **–•—Ä–∞–Ω–µ–Ω–∏–µ** | In-memory dict | Redis + –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å |
| **–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è** | –ú–æ–Ω–æ–ª–∏—Ç–Ω–∞—è | –ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–∞—è —Å –∏–∑–æ–ª—è—Ü–∏–µ–π |
| **–°–µ—Ç–∏** | –ï–¥–∏–Ω–∞—è —Å–µ—Ç—å | –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ç–∏ –ø–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—é |
| **–ü–æ—Ä—Ç—ã** | –û—Ç–∫—Ä—ã—Ç—ã–µ –Ω–∞—Ä—É–∂—É | –¢–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ |
| **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å** | –°–ª–∞–±–∞—è | –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è + –∏–∑–æ–ª—è—Ü–∏—è |
| **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** | –ß–∞—Å—Ç–∏—á–Ω—ã–π | –ü–æ–ª–Ω—ã–π —Å—Ç–µ–∫ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ |
| **–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏** | –ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã –≤–µ—Ä—Å–∏–π | –ï–¥–∏–Ω–∞—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è |
| **–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ** | Multi-stage –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç | Multi-stage —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π |

---

## üöÄ –ü–ª–∞–Ω –≤–Ω–µ–¥—Ä–µ–Ω–∏—è

### –§–∞–∑–∞ 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (1-2 –¥–Ω—è)
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É tasks.py
2. ‚úÖ –ò—Å–ø—Ä–∞–≤–∏—Ç—å API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å Celery
3. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å Dockerfile —Å multi-stage
4. ‚úÖ –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å requirements.txt

### –§–∞–∑–∞ 2: –ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (2-3 –¥–Ω—è)
1. üîß –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ—Ç–∏
2. üîß –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
3. üîß –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
4. üîß –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–ª—è—Ü–∏—é

### –§–∞–∑–∞ 3: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (1-2 –¥–Ω—è)
1. üß™ –û–±–Ω–æ–≤–∏—Ç—å —Ç–µ—Å—Ç—ã –ø–æ–¥ –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
2. üß™ –ù–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
3. üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
4. üß™ Security audit

---

## üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç

–ü–æ—Å–ª–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –≤—Å–µ—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π AI Agent Farm —Å—Ç–∞–Ω–µ—Ç:

- ‚úÖ **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π** —Å–∏—Å—Ç–µ–º–æ–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π Celery –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
- ‚úÖ **–ë–µ–∑–æ–ø–∞—Å–Ω–æ–π** —Å –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ –∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
- ‚úÖ **–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ä—É–µ–º–æ–π** —Å –ø–æ–ª–Ω—ã–º observability —Å—Ç–µ–∫–æ–º
- ‚úÖ **–û—Ç–∫–∞–∑–æ—É—Å—Ç–æ–π—á–∏–≤–æ–π** —Å –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º
- ‚úÖ **–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ-–≥–æ—Ç–æ–≤–æ–π** –¥–ª—è enterprise –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

---

**–°—Ç–∞—Ç—É—Å:** üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –≤—ã—è–≤–ª–µ–Ω—ã, –ø–ª–∞–Ω –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–æ—Ç–æ–≤  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üö® –í—ã—Å–æ–∫–∏–π - –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è  
**–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏:** 5-7 –¥–Ω–µ–π –ø–æ–ª–Ω–æ–π —Ä–∞–±–æ—Ç—ã

